{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.5"
    },
    "colab": {
      "name": "[TALLER] Customized loss functions and regularization with Keras.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yuaquintero/DeepLearning/blob/master/%5BTALLER%5D_Customized_loss_functions_and_regularization_with_Keras.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d4p-KPq249CG",
        "colab_type": "text"
      },
      "source": [
        "## Loading the Fashion MNIST database..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jm-MjZqh49CM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import gzip\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings; warnings.simplefilter('ignore')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zAKkhqYX49Cb",
        "colab_type": "code",
        "outputId": "fb64b020-8f49-49ee-854c-0f0c12779378",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        }
      },
      "source": [
        "from tensorflow.keras import datasets\n",
        "(x_train, y_train), (x_test, y_test) = datasets.fashion_mnist.load_data()\n",
        "X_train = x_train.reshape(x_train.shape[0],x_train.shape[1]*x_train.shape[2])\n",
        "X_test = x_test.reshape(x_test.shape[0],x_test.shape[1]*x_test.shape[2])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "32768/29515 [=================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26427392/26421880 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "8192/5148 [===============================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4423680/4422102 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tZl9XPkx8PWo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pip install tensorflow==1.13rc0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QO9ZHuic49Cx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install keras"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jutQgdU_49DA",
        "colab_type": "code",
        "outputId": "2564846f-0c5b-45a7-999b-017af3a247ab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "from keras.utils import np_utils\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "input_dim = X_train.shape[1]\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_trainN = scaler.fit_transform(X_train)\n",
        "X_testN = scaler.transform(X_test)\n",
        "\n",
        "# convert list of labels to binary class matrix\n",
        "y_trainOHE = np_utils.to_categorical(y_train)\n",
        "nb_classes = y_trainOHE.shape[1]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mSNIjLJJ49DF",
        "colab_type": "text"
      },
      "source": [
        "## Exercises\n",
        "\n",
        "### 1.1 Regularization\n",
        "\n",
        "1.1.1 Define a new model using the keras sequential API. The model must have four hidden layers with the following neurons [128,64,32,16]. For all the hidden layers use the 'relu' activation function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o0xx7mzO49DG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from numpy.random import seed\n",
        "from tensorflow.keras import regularizers\n",
        "seed(1)\n",
        "#tf.random.set_seed(2)\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Activation\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(128, input_dim=input_dim))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dense(64))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dense(32))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dense(16))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dense(nb_classes, activation='softmax'))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KF-LhY1h49DU",
        "colab_type": "text"
      },
      "source": [
        "Run the following cell to train and test the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jzjJrYh949DY",
        "colab_type": "code",
        "outputId": "8745fbd5-464b-498d-c765-028443c0bbab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from tensorflow.keras import optimizers\n",
        "\n",
        "# or instantiate an optimizer before passing it to model.compile\n",
        "sgd = optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=sgd,metrics=['acc'])\n",
        "history= model.fit(X_trainN[:500,:], y_trainOHE[:500,:], epochs=1000, batch_size=16, validation_split=0, verbose=0)\n",
        "\n",
        "preds = model.predict(X_testN, verbose=0)\n",
        "preds = np.argmax(preds,axis=1)\n",
        "Accuracy = np.mean(preds == y_test)\n",
        "print('Accuracy = ', Accuracy*100, '%')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy =  78.46 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rcNsBAhd49Dg",
        "colab_type": "text"
      },
      "source": [
        "Create a graph with the histogram of the network weigths in the first hidden layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BSiHfBfyAb6O",
        "colab_type": "code",
        "outputId": "3fcb7272-8935-4908-c0a9-7adc9dc4cf33",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "weight_0=model.layers[0].get_weights()[0]\n",
        "len(weight_0)\n",
        "weight_0.shape[0]\n",
        "#np.squeeze(W)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "784"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MsIqu2m0plKG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.imshow(weight_0-model.layers[0].get_weights()[0], cmap='gray');"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D7ixCGP2qT5t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot_value_array(weight_0-model.layers[0].get_weights()[0], cmap='gray')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QSRw5JZkFua-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "len(model.layers)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jM2qKmYmJ5cu",
        "colab_type": "code",
        "outputId": "10e3df95-e1ef-45f6-a0a1-cf7004370f3f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 246
        }
      },
      "source": [
        "weights=model.layers[2].get_weights()[0]\n",
        "\n",
        "print(weights)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 0.11882945 -0.01498369 -0.09627794 ... -0.18894364  0.15332371\n",
            "  -0.10029481]\n",
            " [-0.14417073 -0.00185425 -0.09237389 ... -0.18656796  0.06119299\n",
            "  -0.12092488]\n",
            " [ 0.08765567 -0.05115098  0.01132738 ... -0.01604713  0.0366763\n",
            "  -0.04896652]\n",
            " ...\n",
            " [ 0.10220526 -0.04267174 -0.05175226 ...  0.02114982  0.00586397\n",
            "   0.03611135]\n",
            " [-0.07716034  0.1200362   0.26986492 ... -0.07792313 -0.10976321\n",
            "   0.15365523]\n",
            " [-0.06025179 -0.1370014   0.21084313 ...  0.10295838 -0.04754204\n",
            "  -0.00142164]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n8P_ktpV49Di",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt....."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iK45p-PV49Dn",
        "colab_type": "text"
      },
      "source": [
        "1.1.2 Modify the former model to include $L_2$ regularization to every layer of the former model. Define a regularization parameter equal to 0.0001. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mwKZGUH049Do",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "seed(1)\n",
        "tf.random.set_seed(2)\n",
        "\n",
        "#del model\n",
        "#...."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "inPCizJ949Dr",
        "colab_type": "text"
      },
      "source": [
        "Run the following cell to train and test the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dd2iAYX049Dt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# or instantiate an optimizer before passing it to model.compile\n",
        "sgd = optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=sgd)\n",
        "model.fit(X_trainN[:500,:], y_trainOHE[:500,:], epochs=1000, batch_size=16, validation_split=0, verbose=0)\n",
        "\n",
        "preds = model.predict(X_testN, verbose=0)\n",
        "preds = np.argmax(preds,axis=1)\n",
        "Accuracy = np.mean(preds == y_test)\n",
        "print('Accuracy = ', Accuracy*100, '%')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xK1wraXK49D1",
        "colab_type": "text"
      },
      "source": [
        "Create a graph with the histogram of the network weigths in the first hidden layer. Compare it with the histogram obtained in the previous exercise. Is there any effect due to the regularization?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "enEBkm1d49D5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt....."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UiWU3anE49D-",
        "colab_type": "text"
      },
      "source": [
        "1.1.3 Define a new model using the keras sequential API including $L_1$ and $L_2$ regularization methods for every layer. Define a regularization parameter equal to 0.0001 for both regularization terms."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bf-koe8q49D_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "seed(1)\n",
        "tf.random.set_seed(2)\n",
        "\n",
        "del model\n",
        "...."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ACyFSck649ED",
        "colab_type": "text"
      },
      "source": [
        "Run the following cell to train the model and estimate the accuracy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KOC0F3Eq49EF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# or instantiate an optimizer before passing it to model.compile\n",
        "sgd = optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=sgd)\n",
        "model.fit(X_trainN[:500,:], y_trainOHE[:500,:], epochs=1000, batch_size=16, validation_split=0, verbose=0)\n",
        "\n",
        "preds = model.predict(X_testN, verbose=0)\n",
        "preds = np.argmax(preds,axis=1)\n",
        "Accuracy = np.mean(preds == y_test)\n",
        "print('Accuracy = ', Accuracy*100, '%')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "osfPNP3G49EI",
        "colab_type": "text"
      },
      "source": [
        "Create a graph with the histogram of the network weigths in the first hidden layer. Compare it with the histograms obtained in the previous exercises. What is the effect of applying $L_1$ regularization?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CgkHu1Fy49EJ",
        "colab_type": "text"
      },
      "source": [
        "### 1.2 Customized loss function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_w63m8SS49EK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Example MSE\n",
        "from tensorflow.keras import backend as K\n",
        "\n",
        "def mean_squared_error(y_true, y_pred):\n",
        "    return K.mean(K.square(y_pred - y_true), axis=-1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lFNzSJss49ER",
        "colab_type": "text"
      },
      "source": [
        "1.2.1 Use the backend component of keras (https://keras.io/backend/) to define the following loss function and use it to train the model.\n",
        "\n",
        "$$\\mathcal{L}({\\bf{\\hat{y}}},{\\bf{y}}) = -\\frac{1}{N}\\sum_{i=1}^N \\sum_{j=1}^C {\\bf{1}}_{y_i \\in C_j} w_{j}\\log p_{model}[y_i \\in C_j]$$\n",
        "\n",
        "which corresponds to a weighted version of the categorical cross entropy loss function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "itnW06Fk49EW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras import backend as K\n",
        "def weighted_categorical_crossentropy(weights):\n",
        "    ...\n",
        "\n",
        "        \n",
        "    def loss(y_true, y_pred):\n",
        "        ...\n",
        "        return loss\n",
        "    \n",
        "    return loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x0_jB-9l49EZ",
        "colab_type": "text"
      },
      "source": [
        "1.2.2 Use the weighted categorical cross entropy function to train the MLP model with 3 layers defined at the begining of this document. Use the following weights = np.array([1,1,1,1,1,1,4,1,1,1]). Evaluate the model with the test dataset and plot the confusion matrix. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rhNxGDq849Ea",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "weights = np.array([1,1,1,1,1,1,4,1,1,1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J4KrEn-k49Ed",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "del model\n",
        "..."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZuQpTrPz49Ef",
        "colab_type": "text"
      },
      "source": [
        "Train and validate the model. Compare the confusion matrix obtained using the weighted loss function with one obtained in the former class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3UjTev8e49Eg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.fit...\n",
        "\n",
        "cm = confusion...\n",
        "\n",
        "...\n",
        "plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "plt.title('Normalized confusion matrix')\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}